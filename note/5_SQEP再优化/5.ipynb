{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SQEP 的性能再优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import io\n",
    "import redis\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "font_path = '/Volumes/share/data/WBQ/temp/SimHei.ttf'  # 替换为SimHei.ttf的实际路径\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "plt.rcParams['font.family'] = 'SimHei'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClickHouse表结构创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClickHouse配置\n",
    "CLICKHOUSE_HOST = \"localhost\"\n",
    "CLICKHOUSE_PORT = 9000\n",
    "CLICKHOUSE_DB = \"test_data_json_csv\"\n",
    "\n",
    "# 初始化ClickHouse客户端\n",
    "client = Client(host=CLICKHOUSE_HOST, port=CLICKHOUSE_PORT, database=CLICKHOUSE_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除已存在的表（如果有）\n",
    "client.execute(\"DROP TABLE IF EXISTS bar_minute_json\")\n",
    "client.execute(\"DROP TABLE IF EXISTS bar_minute_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建JSON格式表\n",
    "json_table_query = \"\"\"\n",
    "CREATE TABLE bar_minute_json (\n",
    "    symbol Int32,\n",
    "    frame DateTime,\n",
    "    open Float64,\n",
    "    high Float64,\n",
    "    low Float64,\n",
    "    close Float64,\n",
    "    vol Float64,\n",
    "    amount Float64\n",
    ") ENGINE = MergeTree()\n",
    "PARTITION BY toYYYYMM(frame)\n",
    "ORDER BY (symbol, frame);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建CSV格式表\n",
    "csv_table_query = \"\"\"\n",
    "CREATE TABLE bar_minute_csv (\n",
    "    symbol Int32,\n",
    "    frame DateTime,\n",
    "    open Float64,\n",
    "    high Float64,\n",
    "    low Float64,\n",
    "    close Float64,\n",
    "    vol Float64,\n",
    "    amount Float64\n",
    ") ENGINE = MergeTree()\n",
    "PARTITION BY toYYYYMM(frame)\n",
    "ORDER BY (symbol, frame);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表结构创建完成\n"
     ]
    }
   ],
   "source": [
    "# 执行创建表操作\n",
    "client.execute(json_table_query)\n",
    "client.execute(csv_table_query)\n",
    "print(\"表结构创建完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将股票代码和交易所编码为整数\n",
    "def encode_symbol(code, exchange):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        code: 股票代码，如 '000001'\n",
    "        exchange: 交易所，'SH'或'SZ'\n",
    "        \n",
    "    Returns:\n",
    "        整型编码的股票代码\n",
    "    \"\"\"\n",
    "    if exchange.upper() == 'SH':\n",
    "        prefix = '1'\n",
    "    elif exchange.upper() == 'SZ':\n",
    "        prefix = '2'\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的交易所: {exchange}\")\n",
    "        \n",
    "    return int(prefix + code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试数据\n",
    "def generate_test_data(num_records, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_records: 要生成的记录数\n",
    "        batch_size: 每批生成的记录数\n",
    "        \n",
    "    Returns:\n",
    "        生成的数据列表\n",
    "    \"\"\"\n",
    "    # 股票代码列表\n",
    "    stock_codes = [f\"{i:06d}\" for i in range(1, 5001)]  # 5000支股票\n",
    "    exchanges = ['SH', 'SZ']\n",
    "    \n",
    "    # 生成交易日期和时间\n",
    "    start_date = datetime(2023, 1, 1, 9, 30, 0)\n",
    "    \n",
    "    # 生成数据\n",
    "    all_data = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in range(0, num_records, batch_size):\n",
    "        batch_data = []\n",
    "        current_size = min(batch_size, num_records - batch)\n",
    "        \n",
    "        for i in range(current_size):\n",
    "            # 随机选择股票和交易所\n",
    "            stock_code = random.choice(stock_codes)\n",
    "            exchange = random.choice(exchanges)\n",
    "            symbol = encode_symbol(stock_code, exchange)\n",
    "            \n",
    "            # 生成随机时间（在交易时间内）\n",
    "            days_offset = random.randint(0, 365)\n",
    "            minutes_offset = random.randint(0, 240)  # 4小时交易时间\n",
    "            frame = start_date + timedelta(days=days_offset, minutes=minutes_offset)\n",
    "            \n",
    "            # 生成OHLC数据\n",
    "            base_price = random.uniform(10.0, 100.0)\n",
    "            price_range = base_price * 0.02  # 2%的价格波动\n",
    "            \n",
    "            open_price = base_price\n",
    "            close_price = base_price + random.uniform(-price_range, price_range)\n",
    "            high_price = max(open_price, close_price) + random.uniform(0, price_range)\n",
    "            low_price = min(open_price, close_price) - random.uniform(0, price_range)\n",
    "            \n",
    "            vol = random.uniform(10000, 1000000)\n",
    "            amount = vol * ((open_price + close_price) / 2) * random.uniform(0.9, 1.1)\n",
    "            \n",
    "            # 创建记录\n",
    "            record = {\n",
    "                'symbol': symbol,\n",
    "                'frame': frame.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'open': round(open_price, 2),\n",
    "                'high': round(high_price, 2),\n",
    "                'low': round(low_price, 2),\n",
    "                'close': round(close_price, 2),\n",
    "                'vol': round(vol, 2),\n",
    "                'amount': round(amount, 2)\n",
    "            }\n",
    "            \n",
    "            batch_data.append(record)\n",
    "        \n",
    "        all_data.extend(batch_data)\n",
    "        \n",
    "        # 打印进度\n",
    "        if (batch + batch_size) % (num_records // 10) == 0 or batch + batch_size >= num_records:\n",
    "            progress = min(100, (batch + batch_size) / num_records * 100)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"生成进度: {progress:.1f}%, 已生成 {len(all_data)} 条记录, 耗时: {elapsed:.2f}秒\")\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据保存为JSON格式\n",
    "def save_as_json(data, output_file):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: 要保存的数据\n",
    "        output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "    print(f\"JSON格式保存完成，文件大小: {file_size:.2f} MB, 耗时: {elapsed:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据保存为CSV格式\n",
    "def save_as_csv(data, output_file):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: 要保存的数据\n",
    "        output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 获取字段名\n",
    "    fieldnames = list(data[0].keys())\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # 不写入字段名，因为我们要测试不带key的CSV\n",
    "        for record in data:\n",
    "            writer.writerow([record[field] for field in fieldnames])\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "    print(f\"CSV格式保存完成，文件大小: {file_size:.2f} MB, 耗时: {elapsed:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成测试数据...\n",
      "生成进度: 10.0%, 已生成 100000 条记录, 耗时: 0.89秒\n",
      "生成进度: 20.0%, 已生成 200000 条记录, 耗时: 1.91秒\n",
      "生成进度: 30.0%, 已生成 300000 条记录, 耗时: 3.00秒\n",
      "生成进度: 40.0%, 已生成 400000 条记录, 耗时: 4.04秒\n",
      "生成进度: 50.0%, 已生成 500000 条记录, 耗时: 5.11秒\n",
      "生成进度: 60.0%, 已生成 600000 条记录, 耗时: 6.19秒\n",
      "生成进度: 70.0%, 已生成 700000 条记录, 耗时: 7.26秒\n",
      "生成进度: 80.0%, 已生成 800000 条记录, 耗时: 8.31秒\n",
      "生成进度: 90.0%, 已生成 900000 条记录, 耗时: 9.37秒\n",
      "生成进度: 100.0%, 已生成 1000000 条记录, 耗时: 10.42秒\n",
      "\n",
      "保存为JSON格式...\n",
      "JSON格式保存完成，文件大小: 145.02 MB, 耗时: 5.58秒\n",
      "\n",
      "保存为CSV格式...\n",
      "CSV格式保存完成，文件大小: 70.63 MB, 耗时: 4.24秒\n",
      "\n",
      "数据生成完成\n"
     ]
    }
   ],
   "source": [
    "# 创建输出目录\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "# 生成测试数据\n",
    "print(\"开始生成测试数据...\")\n",
    "test_data = generate_test_data(1000000)  # 生成1亿条记录用于测试\n",
    "    \n",
    "# 保存为不同格式\n",
    "print(\"\\n保存为JSON格式...\")\n",
    "save_as_json(test_data, \"data/test_data.json\")\n",
    "    \n",
    "print(\"\\n保存为CSV格式...\")\n",
    "save_as_csv(test_data, \"data/test_data.csv\")\n",
    "    \n",
    "print(\"\\n数据生成完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据生产者 —— JSON格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"quantide666\"  # 添加Redis密码\n",
    "REDIS_QUEUE_NAME = \"bar_minute_json_queue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Redis连接\n",
    "redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def produce_data(redis_client, input_file, batch_size=10000):\n",
    "    \"\"\"将JSON格式数据推送到Redis队列\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入文件路径\n",
    "        batch_size: 每批处理的记录数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 清空队列\n",
    "    redis_client.delete(REDIS_QUEUE_NAME)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_records = 0\n",
    "    batch_records = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line.strip())\n",
    "            batch_records.append(record)\n",
    "            \n",
    "            if len(batch_records) >= batch_size:\n",
    "                # 创建数据包\n",
    "                data_package = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"format\": \"json\",\n",
    "                    \"batch_id\": total_records // batch_size + 1,\n",
    "                    \"records\": batch_records\n",
    "                }\n",
    "                \n",
    "                # 推送到Redis\n",
    "                redis_client.lpush(REDIS_QUEUE_NAME, json.dumps(data_package))\n",
    "                \n",
    "                total_records += len(batch_records)\n",
    "                batch_records = []\n",
    "                \n",
    "                # 打印进度\n",
    "                if total_records % (batch_size * 100) == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"已推送 {total_records} 条记录, 耗时: {elapsed:.2f}秒, 速率: {total_records/elapsed:.2f} 条/秒\")\n",
    "    \n",
    "    # 处理剩余的记录\n",
    "    if batch_records:\n",
    "        data_package = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"format\": \"json\",\n",
    "            \"batch_id\": total_records // batch_size + 1,\n",
    "            \"records\": batch_records\n",
    "        }\n",
    "        redis_client.lpush(REDIS_QUEUE_NAME, json.dumps(data_package))\n",
    "        total_records += len(batch_records)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nJSON数据推送完成，共 {total_records} 条记录\")\n",
    "    print(f\"总耗时: {total_time:.2f}秒, 平均速率: {total_records/total_time:.2f} 条/秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已推送 1000000 条记录, 耗时: 4.81秒, 速率: 207904.87 条/秒\n",
      "已推送 2000000 条记录, 耗时: 10.09秒, 速率: 198150.78 条/秒\n",
      "已推送 3000000 条记录, 耗时: 15.08秒, 速率: 198964.02 条/秒\n",
      "已推送 4000000 条记录, 耗时: 20.16秒, 速率: 198437.68 条/秒\n",
      "已推送 5000000 条记录, 耗时: 25.53秒, 速率: 195852.91 条/秒\n",
      "已推送 6000000 条记录, 耗时: 31.14秒, 速率: 192695.91 条/秒\n",
      "已推送 7000000 条记录, 耗时: 36.50秒, 速率: 191789.71 条/秒\n",
      "已推送 8000000 条记录, 耗时: 41.90秒, 速率: 190946.45 条/秒\n",
      "已推送 9000000 条记录, 耗时: 46.96秒, 速率: 191635.71 条/秒\n",
      "已推送 10000000 条记录, 耗时: 52.07秒, 速率: 192059.43 条/秒\n",
      "已推送 11000000 条记录, 耗时: 57.34秒, 速率: 191839.01 条/秒\n",
      "已推送 12000000 条记录, 耗时: 62.86秒, 速率: 190914.00 条/秒\n",
      "已推送 13000000 条记录, 耗时: 68.67秒, 速率: 189323.47 条/秒\n",
      "已推送 14000000 条记录, 耗时: 74.21秒, 速率: 188649.72 条/秒\n",
      "已推送 15000000 条记录, 耗时: 79.49秒, 速率: 188710.85 条/秒\n",
      "已推送 16000000 条记录, 耗时: 84.90秒, 速率: 188449.27 条/秒\n",
      "已推送 17000000 条记录, 耗时: 90.54秒, 速率: 187765.46 条/秒\n",
      "已推送 18000000 条记录, 耗时: 96.75秒, 速率: 186039.79 条/秒\n",
      "已推送 19000000 条记录, 耗时: 102.34秒, 速率: 185659.40 条/秒\n",
      "已推送 20000000 条记录, 耗时: 107.82秒, 速率: 185501.58 条/秒\n",
      "已推送 21000000 条记录, 耗时: 113.29秒, 速率: 185360.03 条/秒\n",
      "已推送 22000000 条记录, 耗时: 119.25秒, 速率: 184487.34 条/秒\n",
      "已推送 23000000 条记录, 耗时: 124.67秒, 速率: 184494.39 条/秒\n",
      "已推送 24000000 条记录, 耗时: 130.28秒, 速率: 184221.84 条/秒\n",
      "已推送 25000000 条记录, 耗时: 135.88秒, 速率: 183981.07 条/秒\n",
      "已推送 26000000 条记录, 耗时: 141.50秒, 速率: 183739.20 条/秒\n",
      "已推送 27000000 条记录, 耗时: 146.78秒, 速率: 183951.24 条/秒\n",
      "已推送 28000000 条记录, 耗时: 152.13秒, 速率: 184050.56 条/秒\n",
      "已推送 29000000 条记录, 耗时: 157.58秒, 速率: 184037.44 条/秒\n",
      "已推送 30000000 条记录, 耗时: 163.14秒, 速率: 183896.30 条/秒\n",
      "已推送 31000000 条记录, 耗时: 168.54秒, 速率: 183931.75 条/秒\n",
      "已推送 32000000 条记录, 耗时: 174.13秒, 速率: 183767.59 条/秒\n",
      "已推送 33000000 条记录, 耗时: 179.92秒, 速率: 183417.49 条/秒\n",
      "已推送 34000000 条记录, 耗时: 185.84秒, 速率: 182954.61 条/秒\n",
      "已推送 35000000 条记录, 耗时: 191.73秒, 速率: 182546.51 条/秒\n",
      "已推送 36000000 条记录, 耗时: 197.34秒, 速率: 182424.78 条/秒\n",
      "已推送 37000000 条记录, 耗时: 202.79秒, 速率: 182450.67 条/秒\n",
      "已推送 38000000 条记录, 耗时: 208.50秒, 速率: 182251.58 条/秒\n",
      "已推送 39000000 条记录, 耗时: 214.41秒, 速率: 181895.68 条/秒\n",
      "已推送 40000000 条记录, 耗时: 220.08秒, 速率: 181750.75 条/秒\n",
      "已推送 41000000 条记录, 耗时: 225.91秒, 速率: 181490.00 条/秒\n",
      "已推送 42000000 条记录, 耗时: 231.53秒, 速率: 181400.36 条/秒\n",
      "已推送 43000000 条记录, 耗时: 236.96秒, 速率: 181462.08 条/秒\n",
      "已推送 44000000 条记录, 耗时: 242.29秒, 速率: 181597.91 条/秒\n",
      "已推送 45000000 条记录, 耗时: 247.84秒, 速率: 181569.80 条/秒\n",
      "已推送 46000000 条记录, 耗时: 253.51秒, 速率: 181449.14 条/秒\n",
      "已推送 47000000 条记录, 耗时: 259.07秒, 速率: 181417.32 条/秒\n",
      "已推送 48000000 条记录, 耗时: 264.66秒, 速率: 181367.48 条/秒\n",
      "已推送 49000000 条记录, 耗时: 270.00秒, 速率: 181483.21 条/秒\n",
      "已推送 50000000 条记录, 耗时: 275.26秒, 速率: 181643.16 条/秒\n",
      "已推送 51000000 条记录, 耗时: 280.90秒, 速率: 181560.91 条/秒\n",
      "已推送 52000000 条记录, 耗时: 286.50秒, 速率: 181500.36 条/秒\n",
      "已推送 53000000 条记录, 耗时: 292.17秒, 速率: 181403.17 条/秒\n",
      "已推送 54000000 条记录, 耗时: 297.81秒, 速率: 181320.87 条/秒\n",
      "已推送 55000000 条记录, 耗时: 303.31秒, 速率: 181331.05 条/秒\n",
      "已推送 56000000 条记录, 耗时: 308.81秒, 速率: 181342.59 条/秒\n",
      "已推送 57000000 条记录, 耗时: 314.60秒, 速率: 181179.65 条/秒\n",
      "已推送 58000000 条记录, 耗时: 320.38秒, 速率: 181037.52 条/秒\n",
      "已推送 59000000 条记录, 耗时: 325.75秒, 速率: 181118.02 条/秒\n",
      "已推送 60000000 条记录, 耗时: 331.53秒, 速率: 180980.25 条/秒\n",
      "已推送 61000000 条记录, 耗时: 337.21秒, 速率: 180894.09 条/秒\n",
      "已推送 62000000 条记录, 耗时: 343.09秒, 速率: 180708.81 条/秒\n",
      "已推送 63000000 条记录, 耗时: 348.85秒, 速率: 180593.35 条/秒\n",
      "已推送 64000000 条记录, 耗时: 354.65秒, 速率: 180461.05 条/秒\n",
      "已推送 65000000 条记录, 耗时: 360.19秒, 速率: 180462.29 条/秒\n",
      "已推送 66000000 条记录, 耗时: 365.55秒, 速率: 180547.79 条/秒\n",
      "已推送 67000000 条记录, 耗时: 371.30秒, 速率: 180446.68 条/秒\n",
      "已推送 68000000 条记录, 耗时: 376.87秒, 速率: 180434.31 条/秒\n",
      "已推送 69000000 条记录, 耗时: 382.53秒, 速率: 180379.31 条/秒\n",
      "已推送 70000000 条记录, 耗时: 388.23秒, 速率: 180304.91 条/秒\n",
      "已推送 71000000 条记录, 耗时: 394.04秒, 速率: 180186.96 条/秒\n",
      "已推送 72000000 条记录, 耗时: 399.81秒, 速率: 180087.59 条/秒\n",
      "已推送 73000000 条记录, 耗时: 405.61秒, 速率: 179977.82 条/秒\n",
      "已推送 74000000 条记录, 耗时: 411.31秒, 速率: 179913.97 条/秒\n",
      "已推送 75000000 条记录, 耗时: 417.18秒, 速率: 179778.14 条/秒\n",
      "已推送 76000000 条记录, 耗时: 422.83秒, 速率: 179739.37 条/秒\n",
      "已推送 77000000 条记录, 耗时: 428.37秒, 速率: 179750.88 条/秒\n",
      "已推送 78000000 条记录, 耗时: 434.24秒, 速率: 179623.29 条/秒\n",
      "已推送 79000000 条记录, 耗时: 439.70秒, 速率: 179669.13 条/秒\n",
      "已推送 80000000 条记录, 耗时: 445.41秒, 速率: 179611.34 条/秒\n",
      "已推送 81000000 条记录, 耗时: 451.29秒, 速率: 179485.46 条/秒\n",
      "已推送 82000000 条记录, 耗时: 456.84秒, 速率: 179495.41 条/秒\n"
     ]
    }
   ],
   "source": [
    "input = 'data/test_data.json'\n",
    "batch_size = 10000\n",
    "\n",
    "if not os.path.exists(input):\n",
    "    print(f\"错误: 输入文件 {input} 不存在\")\n",
    "    exit(1)\n",
    "    \n",
    "produce_data(redis_client, input, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据生产者 —— CSV格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"quantide666\"  # 添加Redis密码\n",
    "REDIS_QUEUE_NAME = \"bar_minute_csv_queue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Redis连接\n",
    "redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将CSV格式数据推送到Redis队列\n",
    "def produce_data(redis_client,input_file, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_file: 输入文件路径\n",
    "        batch_size: 每批处理的记录数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 清空队列\n",
    "    redis_client.delete(REDIS_QUEUE_NAME)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_records = 0\n",
    "    batch_records = []\n",
    "    \n",
    "    # 字段名列表（与data_generator.py中保持一致）\n",
    "    fieldnames = ['symbol', 'frame', 'open', 'high', 'low', 'close', 'vol', 'amount']\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # CSV不带key，所以我们需要手动映射字段\n",
    "            record = []\n",
    "            for i, value in enumerate(row):\n",
    "                # 根据字段类型进行转换\n",
    "                if i == 0:  # symbol\n",
    "                    record.append(int(value))\n",
    "                elif i == 1:  # frame\n",
    "                    record.append(value)\n",
    "                else:  # 数值字段\n",
    "                    record.append(float(value))\n",
    "            \n",
    "            batch_records.append(record)\n",
    "            \n",
    "            if len(batch_records) >= batch_size:\n",
    "                # 创建数据包\n",
    "                data_package = {\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"format\": \"csv\",\n",
    "                    \"batch_id\": total_records // batch_size + 1,\n",
    "                    \"fieldnames\": fieldnames,\n",
    "                    \"records\": batch_records\n",
    "                }\n",
    "                \n",
    "                # 推送到Redis\n",
    "                redis_client.lpush(REDIS_QUEUE_NAME, json.dumps(data_package))\n",
    "                \n",
    "                total_records += len(batch_records)\n",
    "                batch_records = []\n",
    "                \n",
    "                # 打印进度\n",
    "                if total_records % (batch_size * 100) == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"已推送 {total_records} 条记录, 耗时: {elapsed:.2f}秒, 速率: {total_records/elapsed:.2f} 条/秒\")\n",
    "    \n",
    "    # 处理剩余的记录\n",
    "    if batch_records:\n",
    "        data_package = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"format\": \"csv\",\n",
    "            \"batch_id\": total_records // batch_size + 1,\n",
    "            \"fieldnames\": fieldnames,\n",
    "            \"records\": batch_records\n",
    "        }\n",
    "        redis_client.lpush(REDIS_QUEUE_NAME, json.dumps(data_package))\n",
    "        total_records += len(batch_records)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCSV数据推送完成，共 {total_records} 条记录\")\n",
    "    print(f\"总耗时: {total_time:.2f}秒, 平均速率: {total_records/total_time:.2f} 条/秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'data/test_data.csv'\n",
    "batch_size = 10000\n",
    "    \n",
    "if not os.path.exists(input):\n",
    "    print(f\"错误: 输入文件 {input} 不存在\")\n",
    "    exit(1)\n",
    "    \n",
    "produce_data(redis_client, input, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据消费者 —— JSON格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"quantide666\"  # 添加Redis密码\n",
    "REDIS_QUEUE_NAME = \"bar_minute_json_queue\"\n",
    "\n",
    "# 初始化Redis连接\n",
    "redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClickHouse配置\n",
    "CLICKHOUSE_HOST = \"localhost\"\n",
    "CLICKHOUSE_PORT = 9000\n",
    "CLICKHOUSE_DB = \"test_data_json_csv\"\n",
    "\n",
    "# 初始化ClickHouse客户端\n",
    "client = Client(host=CLICKHOUSE_HOST, port=CLICKHOUSE_PORT, database=CLICKHOUSE_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将JSON格式数据插入到ClickHouse\n",
    "def insert_to_clickhouse(client, data_package):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        client: ClickHouse客户端\n",
    "        data_package: 数据包\n",
    "        \n",
    "    Returns:\n",
    "        插入的记录数\n",
    "    \"\"\"\n",
    "    records = data_package[\"records\"]\n",
    "    if not records:\n",
    "        return 0\n",
    "    \n",
    "    # 准备插入数据\n",
    "    values = []\n",
    "    for record in records:\n",
    "        # 转换日期时间格式\n",
    "        frame = datetime.fromisoformat(record[\"frame\"])\n",
    "        \n",
    "        # 准备行数据\n",
    "        row = (\n",
    "            record[\"symbol\"],\n",
    "            frame,\n",
    "            record[\"open\"],\n",
    "            record[\"high\"],\n",
    "            record[\"low\"],\n",
    "            record[\"close\"],\n",
    "            record[\"vol\"],\n",
    "            record[\"amount\"]\n",
    "        )\n",
    "        values.append(row)\n",
    "    \n",
    "    # 执行插入\n",
    "    query = \"\"\"\n",
    "    INSERT INTO bar_minute_json (\n",
    "        symbol, frame, open, high, low, close, vol, amount\n",
    "    ) VALUES\n",
    "    \"\"\"\n",
    "    \n",
    "    client.execute(query, values)\n",
    "    return len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Redis队列消费数据并插入到ClickHouse\n",
    "def consume_data(redis_client, clickhouse_client, timeout=1, max_records=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        timeout: Redis阻塞超时时间（秒）\n",
    "        max_records: 最大处理记录数，None表示不限制\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_records = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    print(\"开始消费JSON格式数据...\")\n",
    "    \n",
    "    while max_records is None or total_records < max_records:\n",
    "        try:\n",
    "            # 阻塞式获取队列数据\n",
    "            result = redis_client.brpop(REDIS_QUEUE_NAME, timeout=timeout)\n",
    "            if result is None:\n",
    "                # 如果没有获取到数据，说明队列为空，退出循环\n",
    "                print(\"Redis队列为空，停止消费数据。\")\n",
    "                break\n",
    "            \n",
    "            _, json_data = result\n",
    "            data_package = json.loads(json_data)\n",
    "            \n",
    "            # 记录批次开始时间\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # 插入数据\n",
    "            inserted_count = insert_to_clickhouse(clickhouse_client, data_package)\n",
    "            total_records += inserted_count\n",
    "            batch_count += 1\n",
    "            \n",
    "            # 计算批次耗时\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            \n",
    "            # 打印进度\n",
    "            if batch_count % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"已处理 {batch_count} 批次, 共 {total_records} 条记录\")\n",
    "                print(f\"当前批次耗时: {batch_time:.2f}秒, 速率: {inserted_count/batch_time:.2f} 条/秒\")\n",
    "                print(f\"总体耗时: {elapsed:.2f}秒, 平均速率: {total_records/elapsed:.2f} 条/秒\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"数据处理异常: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nJSON数据消费完成，共处理 {batch_count} 批次, {total_records} 条记录\")\n",
    "    print(f\"总耗时: {total_time:.2f}秒, 平均速率: {total_records/total_time:.2f} 条/秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 1  # Redis阻塞超时时间（秒）\n",
    "max_records = None  # 最大处理记录数，None表示不限制\n",
    "\n",
    "consume_data(timeout, max_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据消费者 —— CSV格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试前的准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主测试脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小规模测试（百万级数据）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大规模数据生成脚本（亿级数据）\n",
    "\n",
    "本测试已配置为生成和处理1亿条数据记录，包含5000支不同的股票。主要修改包括：\n",
    "\n",
    "1. 股票数量从100增加到5000\n",
    "2. 数据量从100万增加到1亿\n",
    "3. 批处理大小从1000增加到10000\n",
    "4. 数据生成批次大小从10000增加到100000\n",
    "\n",
    "这些修改确保了测试能够更好地模拟真实环境下的大规模数据处理场景，并提供更加严谨和公平的JSON与CSV格式性能对比结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
